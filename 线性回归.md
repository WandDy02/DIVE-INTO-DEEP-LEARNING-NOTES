# 线性回归

## 1.线性模型

给定一个数据集，我们的目标是寻找模型的权重**w**和偏置b， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过*线性模型*的仿射变换决定，仿射变换由所选权重和偏置确定。

将所有特征放到向量$$\mathbf{x} \in \mathbb{R}^d$$,并将所有权重放到向量$$\mathbf{w} \in \mathbb{R}^d$$,我们可以用点积形式来简洁地表达模型：

$$
\hat{y} = \mathbf{w}^\top \mathbf{x} + b.
$$

## 2.损失函数

*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。当样本$$i$$的预测值为$$\hat{y}^{(i)}$$，其相应的真实标签为$$y^{(i)}$$时， 平方误差可以定义为以下公式：

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$

## 3.解析解

线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。 首先，我们将偏置**b**合并到参数**w**中，合并方法是在包含所有参数的矩阵中附加一列。 我们的预测问题是最小化$$\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$$。 这在损失平面上只有一个临界点，这个临界点对应于整个区域的损失极小点。 将损失关于**w**的导数设为0，得到解析解：

$$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.$$

## 4.随机梯度下降

解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。在无法得到解析解的情况下，我们依然要有效地训练模型，在许多任务中，难以优化的模型效果要更好。所以弄清楚如何训练这些难以优化的模型是非常重要的。



*梯度下降*（gradient descent）这种方法几乎可以优化所有深度学习模型。它通过不断地在损失函数递减的方向上更新参数来降低误差。



**梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做*小批量随机梯度下降*（minibatch stochastic gradient descent）。**



**算法步骤如下：**

**1.初始化模型参数的值，如随机初始化；**

**2.随机抽样一个小批量$$\mathcal{B}$$样本，计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。然后，用梯度乘以一个确定的正数$$\eta$$（学习率）,最后用参数减去这个值来更新参数。**

**3.不断迭代步骤2。**



对于平方损失和仿射变换，我们可以明确地写成如下形式:

$$\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}$$

 $$|\mathcal{B}|$$表示每个小批量中的样本数，这也称为*批量大小*（batch size）。 $$\eta$$表示*学习率*（learning rate）。 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为*超参数*（hyperparameter）。 *调参*（hyperparameter tuning）是选择超参数的过程。
