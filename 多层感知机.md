# 多层感知机

## 1.感知机

### 1.1感知机的定义

感知机(perceptron)是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别。

### 1.2感知机的输出

给定输入$$x$$,权重$$\mathbf w$$,和偏移$$b$$,

$$o=\sigma(<\mathbf{w},\mathbf{x}>+b)$$      $$\sigma(x)=\begin{cases}1&x>0\\-1&otherwise\end{cases} $$

### 1.3感知机的几何解释

感知机的几何解释是线性方程：$$w*x+b=0$$

对应于特征空间 $$R^n$$ 中的一个超平面S，其中w是从超平面的法向量，b是超平面的截距。

这个超平面将特征空间划分为两个部分。位于两部分的点(特征向量)分别被分为**正、负两类**。

因此，超平面S成为分离超平面(separating hyperplane),如图2.1所示。

![image-20230328141746705](.\pictures\图2.1感知机模型.png)

在这张图中，在直线上方的点感知机的输出为1，在直线下方的点感知机的输出为-1。所以感知机的训练就是不断的调整这一条直线，以获得更好的分类效果。

### 1.4感知机的训练

**initialize** w=0 and b=0

**repeat**

**if**  $$y_i[<w,x_i>+b]\leq0$$

**then** $$w\leftarrow\;w+y_ix_i$$ and $$b\;\leftarrow\;y_i$$

**end if until** all classified correctly

等价于使用批量大小为1的梯度下降，并使用如下的损失函数

$$l(y,\mathbf{x},\mathbf{w})=max(0,-y<\mathbf{w},\mathbf{x}>)$$

## 2.多层感知机

感知机只能产生线性分割面，所以无法拟合XOR函数，导致了第一次AI寒冬。多层感知机可以解决这个问题。

## 2.1在网络中加入隐藏层

### 2.1.1线性模型的局限性

线性意味着*单调*假设： 任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）。但是在很多事件中，数据之间的关系会违反单调性。例如用体温去预测死亡率，对于高于37℃的体温来说，体温越高风险越高；但对于低于37℃的体温这一部分来说，体温越高风险越低。如果分开来看两部分都符合单调性，但是整体并不符合单调性。所以无法使用线性模型来预测体温和死亡风险之间的关系。

### 2.1.2从线性到非线性

我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前$$L-1$$层看作表示，把最后一层看作线性预测器。 这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。 下面，我们以图的方式描述了多层感知机

![image-20230328151730440](.\pictures\多层感知机示例.png)

这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。

通过矩阵$$\mathbf{X} \in \mathbb{R}^{n \times d}$$来表示$$n$$个样本的小批量,其中每个样本具有$$d$$个输入特征。对于具有ℎ个隐藏单元的单隐藏层多层感知机， 用$$\mathbf{H} \in \mathbb{R}^{n \times h}$$表示隐藏层的输出，称为*隐藏表示*（hidden representations）。在数学或代码中，$$\mathbf{H}$$也被称为*隐藏层变量*（hidden-layer variable） 或*隐藏变量*（hidden variable）。 因为隐藏层和输出层都是全连接的，所以隐藏层权重$$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$$和隐藏层偏置$$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$$以及输出层权重$$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$$和输出层偏置$$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$$。形式上，我们按如下方式计算单隐藏层多层感知机的输出$$\mathbf{O} \in \mathbb{R}^{n \times q}$$:

$$\begin{split}\begin{aligned}
    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{aligned}\end{split}$$

但如果这样计算输出的话，将$$\mathbf{H}$$代入$$\mathbf{O}$$中，得到：

$$\mathbf{O}  = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{'}$$

仍然是一个线性模型，等价于一个单层模型。

为了发挥多层架构的潜力， 我们还需要一个额外的关键要素： 在仿射变换之后对每个隐藏单元应用非线性的*激活函数*（activation function）$$\sigma$$。 激活函数的输出（例如，$$\sigma$$(⋅)）被称为*活性值*（activations）。 一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：

$$\begin{split}\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}\end{split}$$

最后对得到的$$\mathbf{O}$$进行softmax回归，实现多分类模型。

为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如$$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$$和$$\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$$，一层叠一层，从而产生更有表达能力的模型。

## 2.2常用激活函数

### 2.2.1 ReLU函数

最受欢迎的激活函数是*修正线性单元*（Rectified linear unit，*ReLU*）， 因为它实现简单，同时在各种预测任务中表现良好。 ReLU提供了一种非常简单的非线性变换。给定元素$$x$$，ReLU函数被定义为该元素与0的最大值：

$$\operatorname{ReLU}(x) = \max(x, 0).$$

![image-20230328153647905](.\pictures\ReLU函数.png)

使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。

ReLU函数的导数图像如下图：

![image-20230328153914278](.\pictures\ReLU函数导数.png)

### 2.2.2 sigmoid函数

对于一个定义域在$$\mathbb{R}$$中的输入， *sigmoid函数*将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为*挤压函数*（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：

$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$

当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （sigmoid可以视为softmax的特例）。 然而，sigmoid在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。

![image-20230328154214587](.\pictures\sigmoid函数.png)

sigmoid函数的导数为下面的公式：

$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$

导数图像如下:

![image-20230328154313281](.\pictures\sigmoid函数导数.png)

### 2.2.3 tanh函数

与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：

$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$

图像如下：

![image-20230328154432652](.\pictures\tanh函数.png)

tanh函数的导数是：$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$

导数图像如下:

![image-20230328154533278](.\pictures\hanh函数导数.png)

